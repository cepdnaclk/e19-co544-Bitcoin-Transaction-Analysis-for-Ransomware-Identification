{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyFtI2kum8Mr0tskxVknb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cepdnaclk/e19-co544-Bitcoin-Transaction-Analysis-for-Ransomware-Identification/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1t5V5Pl5-yv1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn import preprocessing, neighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1mhkzmeFbMx",
        "outputId": "dee88b12-d62b-4e04-9754-cc34cdf1ce03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dataset to drive and provide the path here\n",
        "\n",
        "bitcoin_dataset = pd.read_csv('/content/drive/MyDrive/Machine Learning Project/BitcoinHeistData.csv')"
      ],
      "metadata": {
        "id": "R0wastVJFd6a"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the labels to binary labels"
      ],
      "metadata": {
        "id": "BoABmXynFpUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert labels to binary\n",
        "def convert_to_binary(label):\n",
        "    if label == 'white':\n",
        "        return 0  # Assign 0 for 'White' class\n",
        "    else:\n",
        "        return 1  # Assign 1 for 'Ransomware' class\n",
        "\n",
        "# Apply the function to create a new binary label column\n",
        "bitcoin_dataset['Binary_Label'] = bitcoin_dataset['label'].apply(convert_to_binary)\n",
        "\n",
        "# Check for missing values in the target variable\n",
        "missing_target = bitcoin_dataset['Binary_Label'].isna().sum()\n",
        "print(f\"Number of missing values in target: {missing_target}\")\n",
        "\n",
        "# Check the updated DataFrame\n",
        "print(bitcoin_dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftRnL4HFm9y",
        "outputId": "7dd2d5e5-cdd9-4f7a-9c3a-a8c188b66f05"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in target: 0\n",
            "                              address  year  day  length    weight  count  \\\n",
            "0   111K8kZAEnJg245r2cM6y9zgJGHZtJPy6  2017   11      18  0.008333      1   \n",
            "1  1123pJv8jzeFQaCV4w644pzQJzVWay2zcA  2016  132      44  0.000244      1   \n",
            "2  112536im7hy6wtKbpH1qYDWtTyMRAcA2p7  2016  246       0  1.000000      1   \n",
            "3  1126eDRw2wqSkWosjTCre8cjjQW8sSeWH7  2016  322      72  0.003906      1   \n",
            "4  1129TSjKtx65E35GiUo4AYVeyo48twbrGX  2016  238     144  0.072848    456   \n",
            "\n",
            "   looped  neighbors       income            label  Binary_Label  \n",
            "0       0          2  100050000.0  princetonCerber             1  \n",
            "1       0          1  100000000.0   princetonLocky             1  \n",
            "2       0          2  200000000.0  princetonCerber             1  \n",
            "3       0          2   71200000.0  princetonCerber             1  \n",
            "4       0          1  200000000.0   princetonLocky             1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Transformation\n",
        "\n",
        "By comparing skewness and Kurtosis of different transformations, following transformations are applied to features\n",
        "\n",
        "```\n",
        "length - yeojohnson\n",
        "weight - yeojohnson\n",
        "count - boxcox\n",
        "looped - boxcox\n",
        "neighbors - yeojohnson\n",
        "income  - yeojohnson\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QEb1sV6fF2ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis, boxcox, yeojohnson\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\n",
        "\n",
        "\n",
        "# Define transformation functions\n",
        "\n",
        "def boxcox_transform(x):\n",
        "    return boxcox(x + 1e-9)[0]  # Shift values to be positive if necessary\n",
        "\n",
        "def yeojohnson_transform(x):\n",
        "    return yeojohnson(x)[0]\n",
        "\n",
        "# Apply specified transformations to the features\n",
        "bitcoin_dataset['length_transformed'] = yeojohnson_transform(bitcoin_dataset['length'])\n",
        "bitcoin_dataset['weight_transformed'] = yeojohnson_transform(bitcoin_dataset['weight'])\n",
        "bitcoin_dataset['count_transformed'] = boxcox_transform(bitcoin_dataset['count'])\n",
        "bitcoin_dataset['looped_transformed'] = boxcox_transform(bitcoin_dataset['looped'])\n",
        "bitcoin_dataset['neighbors_transformed'] = yeojohnson_transform(bitcoin_dataset['neighbors'])\n",
        "bitcoin_dataset['income_transformed'] = yeojohnson_transform(bitcoin_dataset['income'])"
      ],
      "metadata": {
        "id": "iSM9eLfMF5G4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminate Outliers"
      ],
      "metadata": {
        "id": "8_aVGLSdGUtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Define the threshold for detecting outliers\n",
        "threshold = 3\n",
        "\n",
        "# Calculate Z-scores for each transformed feature\n",
        "z_scores = stats.zscore(bitcoin_dataset[['length_transformed', 'weight_transformed', 'count_transformed', 'looped_transformed', 'neighbors_transformed', 'income_transformed']])\n",
        "\n",
        "# Identify outliers based on Z-scores\n",
        "outliers = (np.abs(z_scores) > threshold).any(axis=1)\n",
        "\n",
        "# # Show the indices of outliers\n",
        "# print(\"Indices of outliers:\", np.where(outliers)[0])\n",
        "\n",
        "# Remove outliers from the dataset\n",
        "bitcoin_dataset_no_outliers = bitcoin_dataset[~outliers]\n",
        "\n",
        "# # Visualize the transformed features after removing outliers\n",
        "# for feature in features:\n",
        "#     plot_transformations(bitcoin_dataset_no_outliers[f'{feature}_transformed'], f'{feature} (No Outliers)')"
      ],
      "metadata": {
        "id": "UEktB4gQGUPu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Dataset (transformed features and outlier handled)"
      ],
      "metadata": {
        "id": "Zd7elTERGhQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the dataset using transformed features with renamed columns\n",
        "bitcoin_df = bitcoin_dataset_no_outliers[['year', 'day', 'length_transformed', 'weight_transformed', 'count_transformed', 'looped_transformed', 'neighbors_transformed', 'income_transformed','Binary_Label']].copy()\n",
        "\n",
        "# Rename the columns\n",
        "bitcoin_df.columns = ['year', 'day', 'length', 'weight', 'count', 'looped', 'neighbors', 'income','label']\n",
        "\n",
        "# Print the DataFrame\n",
        "print(bitcoin_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtSyZ71SGgvy",
        "outputId": "ea0e492e-28db-4db4-b64e-fd2d290bcd91"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   year  day    length    weight         count      looped  neighbors  \\\n",
            "0  2017   11  2.817168  0.008247  1.000000e-09 -993.329103   0.641675   \n",
            "1  2016  132  3.595750  0.000244  1.000000e-09 -993.329103   0.487116   \n",
            "2  2016  246 -0.000000  0.428792  1.000000e-09 -993.329103   0.641675   \n",
            "3  2016  322  4.023807  0.003887  1.000000e-09 -993.329103   0.641675   \n",
            "4  2016  238  4.620371  0.066695  2.583543e+00 -993.329103   0.487116   \n",
            "\n",
            "     income  label  \n",
            "0  4.117890      1  \n",
            "1  4.117884      1  \n",
            "2  4.125573      1  \n",
            "3  4.113623      1  \n",
            "4  4.125573      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling:\n",
        "\n",
        "For the four different models - SVM, Logistic Regression, XGBoost, and Random Forest - the choice of scalers may vary depending on the characteristics of your data and the algorithms themselves. Here's a general guideline for selecting scalers for each model:\n",
        "\n",
        "### SVM:\n",
        "- **Scaler**: MinMaxScaler or StandardScaler.\n",
        "- **Reasoning**: SVM is sensitive to the scale of the features since it uses a distance-based metric to classify data points. Scaling the features to a similar range can improve the performance of SVM. MinMaxScaler may be preferable if you're using SVM with a kernel like RBF, as it bounds the data to a fixed range, preventing large values from dominating the distance calculations.\n",
        "\n",
        "### Logistic Regression:\n",
        "- **Scaler**: StandardScaler.\n",
        "- **Reasoning**: Logistic Regression typically assumes that features are normally distributed. StandardScaler, which scales the data to have a mean of 0 and a standard deviation of 1, aligns with this assumption and is commonly used with logistic regression. It preserves the shape of the distribution and is suitable for linear models.\n",
        "\n",
        "### XGBoost:\n",
        "- **Scaler**: None or MinMaxScaler.\n",
        "- **Reasoning**: XGBoost is a tree-based ensemble method and is inherently robust to the scale of features. You can choose not to scale the features when using XGBoost. However, if you want to scale the features, MinMaxScaler can be used to bound the features to a specific range, ensuring consistency across the trees.\n",
        "\n",
        "### Random Forest:\n",
        "- **Scaler**: None or MinMaxScaler.\n",
        "- **Reasoning**: Similar to XGBoost, Random Forest is a tree-based ensemble method and is not sensitive to the scale of features. You can choose not to scale the features when using Random Forest. However, if you prefer to scale the features, MinMaxScaler can be used for consistency across the trees.\n",
        "\n",
        "### Final Considerations:\n",
        "- For SVM and Logistic Regression, scaling is generally recommended to improve performance.\n",
        "- For tree-based models like XGBoost and Random Forest, scaling is optional and may not significantly impact performance. However, if you prefer consistency or plan to compare results with other models, you can use MinMaxScaler.\n",
        "\n",
        "It's important to experiment with different scalers and evaluate their effects on model performance using cross-validation or other validation techniques to determine the optimal approach for your specific dataset and modeling goals.\n",
        "\n"
      ],
      "metadata": {
        "id": "xDlD8gRFG0pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Scaled dataset"
      ],
      "metadata": {
        "id": "e6LnclnOJAJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select the numerical features to scale\n",
        "numerical_features = ['length', 'weight', 'count', 'looped', 'neighbors', 'income']\n",
        "\n",
        "# Check for missing values in the target variable before scaling\n",
        "missing_target_before = bitcoin_df['label'].isna().sum()\n",
        "print(f\"Number of missing values in target before scaling: {missing_target_before}\")\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Scale the numerical features using StandardScaler\n",
        "standard_scaled_features = standard_scaler.fit_transform(bitcoin_df[numerical_features])\n",
        "\n",
        "# Create a DataFrame with scaled features, ensuring the indices match the original dataframe\n",
        "standard_scaled_df = pd.DataFrame(standard_scaled_features, columns=numerical_features, index=bitcoin_df.index)\n",
        "\n",
        "# Concatenate the scaled features with the non-scaled features\n",
        "bitcoin_df_scaled = pd.concat([bitcoin_df.drop(columns=numerical_features), standard_scaled_df], axis=1)\n",
        "\n",
        "# Check for missing values in the target variable after scaling\n",
        "missing_target_after = bitcoin_df_scaled['label'].isna().sum()\n",
        "print(f\"Number of missing values in target after scaling: {missing_target_after}\")\n",
        "\n",
        "# Verify that the number of missing values in the target variable hasn't changed\n",
        "assert missing_target_before == missing_target_after, \"Mismatch in missing values count before and after scaling.\"\n",
        "\n",
        "# Use the scaled dataset for further processing\n",
        "bitcoin_df = bitcoin_df_scaled\n",
        "\n",
        "# Print the first few rows of the scaled DataFrame\n",
        "print(\"Standard Scaled DataFrame:\")\n",
        "print(bitcoin_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwrg0DE1G0V0",
        "outputId": "ce421ab1-83d9-48d8-ae6a-3a2a05ff2506"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in target before scaling: 0\n",
            "Number of missing values in target after scaling: 0\n",
            "Standard Scaled DataFrame:\n",
            "   year  day  label    length    weight     count    looped  neighbors  \\\n",
            "0  2017   11      1  0.290905 -1.139795 -0.767030 -0.395908   0.527530   \n",
            "1  2016  132      1  0.732148 -1.185167 -0.767030 -0.395908  -1.407922   \n",
            "2  2016  246      1 -1.305660  1.244529 -0.767030 -0.395908   0.527530   \n",
            "3  2016  322      1  0.974739 -1.164512 -0.767030 -0.395908   0.527530   \n",
            "4  2016  238      1  1.312828 -0.808415  1.449737 -0.395908  -1.407922   \n",
            "\n",
            "     income  \n",
            "0 -0.558012  \n",
            "1 -0.558413  \n",
            "2 -0.045499  \n",
            "3 -0.842672  \n",
            "4 -0.045499  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Min-Max Scaled Dataset\n"
      ],
      "metadata": {
        "id": "mU9EFVpyJm5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the numerical features to scale\n",
        "numerical_features = ['length', 'weight', 'count', 'looped', 'neighbors', 'income']\n",
        "\n",
        "# Initialize the scalers\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "# Scale the features using MinMaxScaler\n",
        "minmax_scaled_features = minmax_scaler.fit_transform(bitcoin_df[numerical_features])\n",
        "\n",
        "# Create DataFrames with scaled features\n",
        "minmax_scaled_df = pd.DataFrame(minmax_scaled_features, columns=numerical_features)\n",
        "\n",
        "# Concatenate the scaled features with non-scaled features\n",
        "minmax_scaled_df = pd.concat([bitcoin_df.drop(columns=numerical_features), minmax_scaled_df], axis=1)\n",
        "\n",
        "print(\"MinMax Scaled DataFrame:\")\n",
        "print(minmax_scaled_df.head())\n",
        "\n",
        "# Use MinMax Scaled dataset from this point onwards\n",
        "bitcoin_df = minmax_scaled_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ub5SxR6I_U7",
        "outputId": "45913aa7-36e3-406b-b7f6-04d0d8390ce6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMax Scaled DataFrame:\n",
            "     year    day  label    length    weight     count  looped  neighbors  \\\n",
            "0  2017.0   11.0    1.0  0.609728  0.012518  0.000000     0.0    0.43121   \n",
            "1  2016.0  132.0    1.0  0.778238  0.000370  0.000000     0.0    0.00000   \n",
            "2  2016.0  246.0    1.0  0.000000  0.650865  0.000000     0.0    0.43121   \n",
            "3  2016.0  322.0    1.0  0.870884  0.005900  0.000000     0.0    0.43121   \n",
            "4  2016.0  238.0    1.0  1.000000  0.101237  0.909528     0.0    0.00000   \n",
            "\n",
            "     income  \n",
            "0  0.259310  \n",
            "1  0.259217  \n",
            "2  0.377801  \n",
            "3  0.193497  \n",
            "4  0.377801  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension Reduction(if required):\n",
        "\n",
        "The first principal component explains almost all of the variance in the data (close to 100%), while the subsequent components explain very little variance. In this case, retaining just one principal component would capture the majority of the variance in the data.\n",
        "\n",
        "Therefore, this dataset can be reduced to one dimension."
      ],
      "metadata": {
        "id": "sOgvj5VtHXsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assume bitcoin_df is your DataFrame containing the features\n",
        "# Select the numerical features to apply PCA\n",
        "numerical_features = ['length', 'weight', 'count', 'looped', 'neighbors', 'income']\n",
        "\n",
        "# Extract the numerical features\n",
        "X = bitcoin_df[numerical_features]\n",
        "\n",
        "# Initialize PCA with the desired number of components\n",
        "pca = PCA(n_components=2)  # You can specify the number of components you want to retain\n",
        "\n",
        "# Fit PCA to the feature matrix\n",
        "pca.fit(X)\n",
        "\n",
        "# Transform the feature matrix to its principal components\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "# X_pca will contain the reduced-dimensional representation of the data\n",
        "reduced_df = pd.DataFrame(X_pca)\n",
        "\n",
        "# Use dimension reduced data frame from this point onwards\n",
        "bitcoin_df = reduced_df"
      ],
      "metadata": {
        "id": "QQ8GoioKHXXS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance Dataset with Under Sampling"
      ],
      "metadata": {
        "id": "SivjOO3nJxb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = 'label'\n",
        "\n",
        "# Check for missing values in the target variable\n",
        "missing_target = bitcoin_df[target].isna().sum()\n",
        "print(f\"Number of missing values in target: {missing_target}\")\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = bitcoin_df.drop(columns=[target])\n",
        "y = bitcoin_df[target]\n",
        "\n",
        "# Initialize the resamplers\n",
        "undersample = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# Apply undersampling\n",
        "X_under, y_under = undersample.fit_resample(X, y)\n",
        "\n",
        "# Print the number of samples after resampling\n",
        "print(\"Number of samples after undersampling:\", X_under.shape[0])\n",
        "\n",
        "print(\"\\nClass distribution after undersampling:\")\n",
        "print(pd.Series(y_under).value_counts())\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCw8Q9x-Jwuc",
        "outputId": "1a2c97ab-526c-4e9c-8858-0b5b32b64374"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in target: 0\n",
            "Number of samples after undersampling: 81604\n",
            "\n",
            "Class distribution after undersampling:\n",
            "label\n",
            "0    40802\n",
            "1    40802\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Balance Dataset with a combination of undersampling and oversampling\n",
        "\n"
      ],
      "metadata": {
        "id": "qfj5XRY0LuiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'target' is the name of your target variable column\n",
        "target = 'label'\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = bitcoin_df.drop(columns=[target])\n",
        "y = bitcoin_df[target]\n",
        "\n",
        "# Initialize the resamplers\n",
        "smoteenn = SMOTEENN(random_state=42)\n",
        "\n",
        "# Apply combination (SMOTE + ENN)\n",
        "X_combined, y_combined = smoteenn.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# Print the number of samples after resampling\n",
        "print(\"Number of samples after combination:\", X_combined.shape[0])\n",
        "\n",
        "print(\"\\nClass distribution after combination :\")\n",
        "print(pd.Series(y_combined).value_counts())\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4q48KCcL0WW",
        "outputId": "24d5b506-2632-454f-985d-d0efe29520f0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples after combination: 37116\n",
            "\n",
            "Class distribution after combination :\n",
            "label\n",
            "1    20496\n",
            "0    16620\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance Dataset with oversampling"
      ],
      "metadata": {
        "id": "PNcKsvLnNudf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'target' is the name of your target variable column\n",
        "target = 'label'\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = bitcoin_df.drop(columns=[target])\n",
        "y = bitcoin_df[target]\n",
        "\n",
        "# Initialize the resamplers\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply oversampling (SMOTE)\n",
        "X_over, y_over = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the number of samples after resampling\n",
        "print(\"Number of samples after oversampling (SMOTE):\", X_over.shape[0])\n",
        "\n",
        "print(\"\\nClass distribution after oversampling (SMOTE):\")\n",
        "print(pd.Series(y_over).value_counts())\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkEmKyEPMtGk",
        "outputId": "bbb73137-44e3-48f2-9f7e-46869468eb48"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples after oversampling (SMOTE): 5709906\n",
            "\n",
            "Class distribution after oversampling (SMOTE):\n",
            "label\n",
            "1    2854953\n",
            "0    2854953\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}